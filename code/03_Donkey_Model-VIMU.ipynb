{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dabfb61-8e59-4b51-a5cb-b02d7f12b240",
   "metadata": {},
   "source": [
    "# Donkey Car Attempt\n",
    "\n",
    "My attempts to build a CNN from scratch are hitting walls. I am going to try out a model suggested on the [Donkey Car Webpage](https://docs.donkeycar.com/dev_guide/model/), to establish a \"baseline\" (very broadly) of sorts to which I can compate the \"custom\" models of my own design.\n",
    "\n",
    "Steps:\n",
    "1. Establish modeling parameters\n",
    "1. Split data into training and testing sets\n",
    "1. Scale telemetry data with scikitlearn's minmaxscaler or standardscaler\n",
    "1. Save the scaler to scale fresh driving input data for inferences\n",
    "1. Define methods to construct, fit, plot, and save a model\n",
    "1. Fit, save, and plot a constructed model with a range of batch sizes\n",
    "1. Save the model and record path and metrics/parameters in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a186988-1c64-48d0-984a-6c15b299587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 18:14:39.027021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "import time\n",
    "import pickle\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from modeling_methods import run_model, plot_metrics, save_model, create_donkey_vimu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from keras_tuner import BayesianOptimization, Hyperband, HyperModel\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.backend import concatenate\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, Flatten, Convolution2D\n",
    "from tensorflow.keras.metrics import MAE, MSE, RootMeanSquaredError\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db63f4ff-a88a-47b4-9400-b2c706a22932",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directories\n",
    "working_date = '11_12_2021'\n",
    "working_time = '19_28_18'\n",
    "\n",
    "## Directories\n",
    "data_directory = f'../data/{working_date}/{working_time}'\n",
    "model_directory = f'../models/{working_date}/{working_time}'\n",
    "\n",
    "## File paths\n",
    "cam_input_dataset_file = f'{data_directory}/X_img.npy'\n",
    "telem_input_dataset_file = f'{data_directory}/X_telem.pkl'\n",
    "target_dataset_file = f'{data_directory}/y.npy'\n",
    "\n",
    "## Parameters\n",
    "scaler = 'standard' # ['minmax', 'standard']\n",
    "dual_outputs = True\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "early_stop_patience = 5 # None for no stop\n",
    "epochs = 250\n",
    "\n",
    "create_model = create_donkey_vimu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395f538-7d20-41bd-ad43-903722c14353",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451f5725-213b-4492-bc82-202c86181efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115052, 64, 64, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the datasets\n",
    "X_cam = np.load(cam_input_dataset_file)\n",
    "X_telem = pd.read_pickle(telem_input_dataset_file).to_numpy()\n",
    "y = np.load(target_dataset_file)\n",
    "\n",
    "## Split for dual output\n",
    "if dual_outputs:\n",
    "    y_steering = y[:, 0]\n",
    "    y_throttle = y[:, 1]\n",
    "\n",
    "## Check Shape\n",
    "X_cam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024459d-2092-4448-b76b-1163bac9130b",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30558a03-f187-479b-9fea-eea61290f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dual_outputs:\n",
    "    datasets = train_test_split(X_cam, X_telem, y_steering, y_throttle, test_size=0.1, random_state=0)\n",
    "else:\n",
    "    datasets = train_test_split(X_cam, X_telem, y, test_size=0.1, random_state=0)\n",
    "    \n",
    "X_cam_train = datasets[0]\n",
    "X_cam_test = datasets[1]\n",
    "X_telem_train = datasets[2]\n",
    "X_telem_test = datasets[3]\n",
    "y_train = datasets[4]\n",
    "y_test = datasets[5]\n",
    "    \n",
    "if dual_outputs:\n",
    "    y_st_train = datasets[4]\n",
    "    y_st_test = datasets[5]\n",
    "    y_th_train = datasets[6]\n",
    "    y_th_test = datasets[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001361f5-0f0a-496e-8e26-846e12c0be7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scale IMU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676b1e05-40e4-4cf0-986d-4b5094637c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = None\n",
    "scaler_file = None\n",
    "if scaler == 'minmax':\n",
    "    sc = MinMaxScaler() # default range: [0, 1]\n",
    "    scaler_file = f'{data_directory}/mm_scaler_{time.strftime(\"%m_%d_%H_%M\")}.pkl'\n",
    "elif scaler == 'standard':\n",
    "    sc = StandardScaler()\n",
    "    scaler_file = f'{data_directory}/ss_scaler_{time.strftime(\"%m_%d_%H_%M\")}.pkl'\n",
    "\n",
    "## Fit to then and transform training data\n",
    "X_telem_train_sc = sc.fit_transform(X_telem_train)\n",
    "## Transform testing data\n",
    "X_telem_test_sc = sc.transform(X_telem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a507fa-f488-442a-bcc2-9943507140be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the Scaler for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac8d37d-8624-4631-8246-f8079cb91f28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/11_12_2021/19_28_18/ss_scaler_11_18_18_14.pkl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save as pickle\n",
    "pickle.dump(sc, open(scaler_file, 'wb'))\n",
    "\n",
    "## Print path\n",
    "scaler_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd46a7b-3435-4a05-afcc-274e3af58264",
   "metadata": {},
   "source": [
    "### Get Input Shape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576a58b8-4b6b-4123-b4ad-9a4a83df348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create variables\n",
    "cam_input_shape = X_cam_train[0].shape\n",
    "telem_input_shape = X_telem_train_sc[0].shape\n",
    "\n",
    "## Check telemetry input shape\n",
    "telem_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ada31-38b8-4741-8e56-6c0c8038777b",
   "metadata": {},
   "source": [
    "## Model Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0febfbaa-d815-45d4-9c60-7c069f767e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Batch size 32 start: 18:14:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 18:14:41.521345: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-18 18:14:41.522035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-18 18:14:41.547578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.547703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.797GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-11-18 18:14:41.547740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-18 18:14:41.549030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-18 18:14:41.549074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-11-18 18:14:41.550156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-18 18:14:41.550360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-18 18:14:41.551703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-18 18:14:41.552371: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-18 18:14:41.555378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-18 18:14:41.555489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.555661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.555752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-11-18 18:14:41.556340: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-18 18:14:41.556492: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-18 18:14:41.556586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.556689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.797GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-11-18 18:14:41.556718: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-18 18:14:41.556735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-18 18:14:41.556749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-11-18 18:14:41.556762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-18 18:14:41.556776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-18 18:14:41.556791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-18 18:14:41.556804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-18 18:14:41.556817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-18 18:14:41.556864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.556994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.557079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-11-18 18:14:41.557110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-18 18:14:41.960398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-18 18:14:41.960422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-11-18 18:14:41.960427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-11-18 18:14:41.960608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.960778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.960907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 18:14:41.961018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4865 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2021-11-18 18:14:42.123915: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1696497664 exceeds 10% of free system memory.\n",
      "2021-11-18 18:14:42.807051: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-11-18 18:14:42.827384: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3406150000 Hz\n",
      "2021-11-18 18:14:43.652258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-18 18:14:43.774723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-18 18:14:44.275878: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2021-11-18 18:14:44.340108: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29411/1147926161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                               early_stop_patience=early_stop_patience)\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/capstone/code/modeling_methods.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model_history_file, model, results, batch_size, scaler_file)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmodel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{model_directory}/model_{model_index}.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     history_dictionary = {\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_directory' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = [X_cam_train, X_telem_train_sc]\n",
    "X_test = (X_cam_test, X_telem_test_sc)\n",
    "\n",
    "if dual_outputs:\n",
    "    y_train = (y_st_train, y_th_train)\n",
    "    y_test = (y_st_test, y_th_test)\n",
    "else:\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "\n",
    "## Run models for each batch size\n",
    "print('---')\n",
    "for batch_size in batch_sizes:\n",
    "    print(f'Batch size {batch_size} start: {time.strftime(\"%H:%M:%S\")}')\n",
    "    model = create_model(cam_input_shape, telem_input_shape, dual_outputs)\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(learning_rate=0.0001), \n",
    "                  metrics=['mae', RootMeanSquaredError()])\n",
    "    model, results = run_model(model, X_train, y_train, X_test, y_test, \n",
    "                               batch_size, epochs,\n",
    "                              early_stop_patience=early_stop_patience)\n",
    "    model_path = save_model(model_directory, model, results, batch_size, scaler_file)\n",
    "    history = {k: v for k, v in results.history.items()}\n",
    "    plot_metrics(history, batch_size)\n",
    "    print(f'Batch size {batch_size} end:   {time.strftime(\"%H:%M:%S\")}')\n",
    "    print(f'Epochs run: {len(history[\"loss\"])}')\n",
    "    print(f'path: {model_path}')\n",
    "    print('---')\n",
    "# model_history.tail(len(batch_sizes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
